{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing CUDA Kernels\n",
    "-------------------\n",
    "## Contents\n",
    "- Example - computing pairwise distances given a dataset of points\n",
    "    - Solution using serial for-loops\n",
    "    - Simple CUDA solution\n",
    "    - Optimized CUDA solution\n",
    "- Example - matrix multiplication\n",
    "    - Simple CUDA solution\n",
    "    - Optimized CUDA solution\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the GPU allocated to us using the ```nvidia-smi``` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  2 14:11:09 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    35W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'Tesla V100-SXM2-16GB'                              [SUPPORTED]\n",
      "                      compute capability: 7.0\n",
      "                           pci device id: 0\n",
      "                              pci bus id: 4\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Switch to the `BIOE-488-v2` jupyter kernel **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, cuda, vectorize, guvectorize, stencil\n",
    "from numba import prange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import convolve as scipy_convolve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba.core.errors import NumbaPerformanceWarning\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore', category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 1] Example - Computing Pairwise Distances\n",
    "\n",
    "This computation shows up in many practical scenarios/problems:\n",
    "- Distance matrices for clustering algorithms\n",
    "- k-Nearest Neighbors (kNN) classification algorithm\n",
    "- Estimation of topological manifolds\n",
    "- Similarity-based searches such as recommendations or information retrieval\n",
    "\n",
    "![](./distance_matrix.jpg)\n",
    "\n",
    "Notice that:\n",
    "- The cells of the distance matrix can be filled in parallel since the distance computation (Euclidean distance, for example) has no external dependencies\n",
    "- Typically, the distance between A and B is the same as B and A. Let's ignore this property for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "xy_coordinates = np.random.rand(n_samples, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73853732, 0.23642104],\n",
       "       [0.86074276, 0.39490418],\n",
       "       [0.98584768, 0.83800459],\n",
       "       ...,\n",
       "       [0.4910636 , 0.86815911],\n",
       "       [0.44461157, 0.56629412],\n",
       "       [0.44235231, 0.02568889]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 2] Serial Solution using For-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def distance_matrix_serial(data):\n",
    "    distance_matrix = np.zeros((data.shape[0], data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[0]):\n",
    "            distance_matrix[i, j] = distance.euclidean(data[i], data[j])\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Serial] Starting the timer ---\n",
      "--- Done: The execution took 12.0077223777771 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- [Serial] Starting the timer ---\")\n",
    "result_serial = distance_matrix_serial(xy_coordinates)\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 3] Simple CUDA Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# one block computes one cell inside the distance matrix\n",
    "# single thread inside the block performs the computation\n",
    "\n",
    "@cuda.jit\n",
    "def distance_matrix_cuda(data, distance_matrix):\n",
    "\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    if i > len(data) or j > len(data):\n",
    "        return\n",
    "    \n",
    "    distance_matrix[i, j] = ((data[i,0] - data[j,0])**2 + (data[i,1] - data[j,1])**2)**0.5\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [simple CUDA] Starting the timer ---\n",
      "--- Done: The execution took 0.005281209945678711 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_blocks = (n_samples, n_samples)\n",
    "num_threads_per_block = 1\n",
    "\n",
    "result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA] Starting the timer ---\")\n",
    "result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, result_cuda) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 4] Simple CUDA Solution + Shared CUDA Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one block computes distances from current point to all other points\n",
    "# each point is handled by a separate thread inside the block\n",
    "# threads inside the block take the current point from shared memory of the block\n",
    "\n",
    "from numba import cuda, float64\n",
    "\n",
    "@cuda.jit\n",
    "def distance_matrix_cuda_optimized(data, distance_matrix):\n",
    "\n",
    "    current_point = cuda.shared.array(shape=(2), dtype=float64)\n",
    "\n",
    "    i = cuda.blockIdx.x\n",
    "    if i < data.shape[0]:\n",
    "        current_point = data[i]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "\n",
    "    j = cuda.threadIdx.x\n",
    "    if j < data.shape[0]:\n",
    "        distance_matrix[i, j] = ((current_point[0] - data[j,0])**2 + (current_point[1] - data[j,1])**2) ** 0.5\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [simple CUDA + shared memory] Starting the timer ---\n",
      "--- Done: The execution took 0.004450559616088867 seconds ---\n",
      "\n",
      "Your result is correct!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_blocks = n_samples\n",
    "num_threads_per_block = n_samples\n",
    "\n",
    "result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"\\n--- [simple CUDA + shared memory] Starting the timer ---\")\n",
    "result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, result_cuda_optimized) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the difference with and without shared memory more clearly (100 repeats)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_simple_cuda(n_samples=1000, n_repeats=100):\n",
    "    exec_times = []\n",
    "    for _ in range(n_repeats):\n",
    "        xy_coordinates = np.random.rand(n_samples, 2)\n",
    "        num_blocks = (n_samples, n_samples)\n",
    "        num_threads_per_block = 1\n",
    "        result_cuda = np.zeros((n_samples, n_samples), dtype=float)\n",
    "        start_time = time.time()\n",
    "        distance_matrix_cuda[num_blocks, num_threads_per_block](xy_coordinates, result_cuda); cuda.synchronize()\n",
    "        exec_times.append(time.time() - start_time)\n",
    "    return exec_times\n",
    "\n",
    "def repeat_optimized_cuda(n_samples=1000, n_repeats=100):\n",
    "    exec_times = []\n",
    "    for _ in range(n_repeats):\n",
    "        xy_coordinates = np.random.rand(n_samples, 2)\n",
    "        num_blocks = n_samples\n",
    "        num_threads_per_block = n_samples\n",
    "        result_cuda_optimized = np.zeros((n_samples, n_samples), dtype=float)\n",
    "        start_time = time.time()\n",
    "        distance_matrix_cuda_optimized[num_blocks, num_threads_per_block](xy_coordinates, result_cuda_optimized); cuda.synchronize()\n",
    "        exec_times.append(time.time() - start_time)\n",
    "    return exec_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple CUDA (100 repeats): 0.004269917011260987 (0.0005495242950819542)\n",
      "Optimized CUDA (100 repeats): 0.0027141976356506348 (0.00032631713289847224)\n"
     ]
    }
   ],
   "source": [
    "exec_times_simple_cuda = repeat_simple_cuda()\n",
    "exec_times_optimized_cuda = repeat_optimized_cuda()\n",
    "\n",
    "print(f\"Simple CUDA (100 repeats): {np.mean(exec_times_simple_cuda)} ({np.std(exec_times_simple_cuda)})\")\n",
    "print(f\"Optimized CUDA (100 repeats): {np.mean(exec_times_optimized_cuda)} ({np.std(exec_times_optimized_cuda)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 5] Matrix multiplication - Simple CUDA Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your result is correct!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TPB = 16\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*200, TPB*30) # random matrix\n",
    "B = np.random.rand(TPB*30, TPB*100) # random matrix\n",
    "\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = (TPB, TPB)\n",
    "blocks_x = int(math.ceil(A.shape[0] / num_threads_per_block[0]))\n",
    "blocks_y = int(math.ceil(B.shape[1] / num_threads_per_block[1]))\n",
    "num_blocks = (blocks_x, blocks_y)\n",
    "\n",
    "matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 6] Matrix multiplication - Simple CUDA Solution + Shared CUDA Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32, float64\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float64)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float64)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your result is correct!\n",
      "[[120.773513   111.18349623 120.26918948 ... 123.45017539 122.30226101\n",
      "  119.52684445]\n",
      " [127.63774925 117.27620851 121.53910656 ... 124.62518513 122.652021\n",
      "  126.93094208]\n",
      " [128.05175396 117.00190238 123.32849374 ... 127.94694475 128.79161761\n",
      "  122.92012669]\n",
      " ...\n",
      " [123.34495941 119.6457591  120.85944957 ... 122.93689211 125.75448403\n",
      "  124.1474618 ]\n",
      " [121.40520723 117.47022588 125.75199653 ... 123.92054533 127.22949691\n",
      "  124.47988488]\n",
      " [120.19825041 119.03126012 123.53457496 ... 126.32816158 122.04880435\n",
      "  123.82436751]]\n"
     ]
    }
   ],
   "source": [
    "# Controls threads per block and shared memory usage.\n",
    "\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.random.rand(TPB*200, TPB*30) # random matrix\n",
    "B = np.random.rand(TPB*30, TPB*100) # random matrix\n",
    "\n",
    "C = np.zeros((A.shape[0], B.shape[1])) # output matrix\n",
    "\n",
    "# Configure the blocks\n",
    "num_threads_per_block = (TPB, TPB)\n",
    "blocks_x = int(math.ceil(A.shape[0] / num_threads_per_block[1]))\n",
    "blocks_y = int(math.ceil(B.shape[1] / num_threads_per_block[0]))\n",
    "num_blocks = (blocks_x, blocks_y)\n",
    "\n",
    "fast_matmul[num_blocks, num_threads_per_block](A, B, C); cuda.synchronize()\n",
    "\n",
    "# Check result with serial/CPU version\n",
    "result_serial = np.matmul(A, B)\n",
    "print(\"\\nYour result is {}\".format(\"correct!\" if np.allclose(result_serial, C) else \"incorrect.\"))\n",
    "print(result_serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BIOE-488-v2]",
   "language": "python",
   "name": "conda-env-BIOE-488-v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "353902f3f2f769574ee6d5e609f500cb3c8385ac61494244183cc0b6ad3e28b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
